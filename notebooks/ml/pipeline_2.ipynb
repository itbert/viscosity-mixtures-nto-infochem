{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "from torch.utils.data import DataLoader, TensorDataset\n"
      ],
      "metadata": {
        "id": "gglLJixN3jkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/data.csv')\n",
        "df = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "727p6ln93lAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataPreprocessor:\n",
        "    def __init__(self, cat_cols):\n",
        "        self.cat_cols = cat_cols\n",
        "        self.label_encoders = {}\n",
        "        self.scaler = StandardScaler()\n",
        "\n",
        "    def fit_transform(self, df):\n",
        "        df = df.copy()\n",
        "        df.fillna(0, inplace=True)\n",
        "\n",
        "        for col in self.cat_cols:\n",
        "            df[col] = df[col].astype(str).replace('0', 'MISSING')\n",
        "            le = LabelEncoder()\n",
        "            df[col] = le.fit_transform(df[col])\n",
        "            self.label_encoders[col] = le\n",
        "\n",
        "        num_cols = df.columns[~df.columns.isin([*self.cat_cols, 'target'])]\n",
        "        if len(num_cols) > 0:\n",
        "            df[num_cols] = self.scaler.fit_transform(df[num_cols])\n",
        "\n",
        "        return df\n"
      ],
      "metadata": {
        "id": "aOpdhN1D8AYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class ImprovedModel(nn.Module):\n",
        "    def __init__(self, num_features, cat_dims, embedding_dim=4):\n",
        "        super().__init__()\n",
        "        self.embeddings = nn.ModuleList([\n",
        "            nn.Embedding(dim, embedding_dim) for dim in cat_dims\n",
        "        ])\n",
        "        for emb in self.embeddings:\n",
        "            init.xavier_normal_(emb.weight)\n",
        "\n",
        "        input_size = num_features + len(cat_dims)*embedding_dim\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(input_size, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.LeakyReLU(0.1),\n",
        "\n",
        "            nn.Linear(128, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.LeakyReLU(0.1),\n",
        "\n",
        "            nn.Linear(64, 32),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.LeakyReLU(0.1),\n",
        "\n",
        "            nn.Linear(32, 1)\n",
        "        )\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
        "                if m.bias is not None:\n",
        "                    init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x_num, x_cat):\n",
        "        embeddings = []\n",
        "        for i, emb_layer in enumerate(self.embeddings):\n",
        "            embeddings.append(emb_layer(x_cat[:, i]))\n",
        "        embeddings = torch.cat(embeddings, dim=1)\n",
        "\n",
        "        x = torch.cat([x_num, embeddings], dim=1)\n",
        "        return self.fc(x).squeeze()\n"
      ],
      "metadata": {
        "id": "20x-ou1p8JCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CAT_COLS = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "TARGET = 'oil_property_value'\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 200\n",
        "\n",
        "preprocessor = DataPreprocessor(CAT_COLS)\n",
        "processed_df = preprocessor.fit_transform(df)\n",
        "\n",
        "X_num = processed_df.drop(CAT_COLS + [TARGET], axis=1).values.astype(np.float32)\n",
        "X_cat = processed_df[CAT_COLS].values.astype(np.int64)\n",
        "y = processed_df[TARGET].values.astype(np.float32)\n",
        "\n",
        "y_scaler = StandardScaler()\n",
        "y = y_scaler.fit_transform(y.reshape(-1, 1)).flatten()\n",
        "\n",
        "X_num_train, X_num_val, X_cat_train, X_cat_val, y_train, y_val = train_test_split(\n",
        "    X_num, X_cat, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "3y4TZYvS8LNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TensorDataset(\n",
        "    torch.tensor(X_num_train),\n",
        "    torch.tensor(X_cat_train),\n",
        "    torch.tensor(y_train)\n",
        ")\n",
        "val_dataset = TensorDataset(\n",
        "    torch.tensor(X_num_val),\n",
        "    torch.tensor(X_cat_val),\n",
        "    torch.tensor(y_val)\n",
        ")"
      ],
      "metadata": {
        "id": "cTM-Md6k8cf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "cat_dims = [len(preprocessor.label_encoders[col].classes_) for col in CAT_COLS]\n",
        "model = ImprovedModel(num_features=X_num.shape[1], cat_dims=cat_dims)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n"
      ],
      "metadata": {
        "id": "Orhm9D3N8dJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "criterion = nn.SmoothL1Loss()\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "patience = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for x_num, x_cat, y_batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(x_num, x_cat)\n",
        "        loss = criterion(preds, y_batch)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for x_num, x_cat, y_batch in val_loader:\n",
        "            preds = model(x_num, x_cat)\n",
        "            val_loss += criterion(preds, y_batch).item()\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    val_loss /= len(val_loader)\n",
        "\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    total_mae = 0\n",
        "    with torch.no_grad():\n",
        "        for x_num_val, x_cat_val, y_val in val_loader:\n",
        "            val_preds = model(x_num_val, x_cat_val)\n",
        "\n",
        "            val_loss += criterion(val_preds, y_val).item()\n",
        "            mae = torch.mean(torch.abs(val_preds - y_val))\n",
        "            total_mae += mae.item() * y_val.size(0)\n",
        "\n",
        "    # Вычисляем средние значения\n",
        "    val_loss_epoch = val_loss / len(val_loader)\n",
        "    val_mae = total_mae / len(val_loader.dataset)\n",
        "\n",
        "    print(f'Epoch {epoch+1:03d} | '\n",
        "          f'Train Loss: {train_loss_epoch:.4f} | '\n",
        "          f'Val Loss: {val_loss_epoch:.4f} | '\n",
        "          f'Val MAE: {val_mae:.4f}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rFo4K_T8hAp",
        "outputId": "038e38f5-a9b1-4372-9730-ec3c10f077c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | Train Loss: 0.0045 | Val Loss: 0.0044 | Val MAE: 0.0646\n",
            "Epoch 002 | Train Loss: 0.0045 | Val Loss: 0.0043 | Val MAE: 0.0622\n",
            "Epoch 003 | Train Loss: 0.0045 | Val Loss: 0.0046 | Val MAE: 0.0651\n",
            "Epoch 004 | Train Loss: 0.0045 | Val Loss: 0.0044 | Val MAE: 0.0647\n",
            "Epoch 005 | Train Loss: 0.0045 | Val Loss: 0.0041 | Val MAE: 0.0626\n",
            "Epoch 006 | Train Loss: 0.0045 | Val Loss: 0.0037 | Val MAE: 0.0591\n",
            "Epoch 007 | Train Loss: 0.0045 | Val Loss: 0.0039 | Val MAE: 0.0588\n",
            "Epoch 008 | Train Loss: 0.0045 | Val Loss: 0.0043 | Val MAE: 0.0627\n",
            "Epoch 009 | Train Loss: 0.0045 | Val Loss: 0.0043 | Val MAE: 0.0624\n",
            "Epoch 010 | Train Loss: 0.0045 | Val Loss: 0.0043 | Val MAE: 0.0639\n",
            "Epoch 011 | Train Loss: 0.0045 | Val Loss: 0.0044 | Val MAE: 0.0657\n",
            "Epoch 012 | Train Loss: 0.0045 | Val Loss: 0.0045 | Val MAE: 0.0674\n",
            "Epoch 013 | Train Loss: 0.0045 | Val Loss: 0.0040 | Val MAE: 0.0632\n",
            "Epoch 014 | Train Loss: 0.0045 | Val Loss: 0.0044 | Val MAE: 0.0643\n",
            "Epoch 015 | Train Loss: 0.0045 | Val Loss: 0.0043 | Val MAE: 0.0625\n",
            "Epoch 016 | Train Loss: 0.0045 | Val Loss: 0.0041 | Val MAE: 0.0601\n",
            "Epoch 017 | Train Loss: 0.0045 | Val Loss: 0.0039 | Val MAE: 0.0605\n",
            "Epoch 018 | Train Loss: 0.0045 | Val Loss: 0.0040 | Val MAE: 0.0620\n",
            "Epoch 019 | Train Loss: 0.0045 | Val Loss: 0.0041 | Val MAE: 0.0618\n",
            "Epoch 020 | Train Loss: 0.0045 | Val Loss: 0.0046 | Val MAE: 0.0673\n",
            "Epoch 021 | Train Loss: 0.0045 | Val Loss: 0.0045 | Val MAE: 0.0637\n",
            "Epoch 022 | Train Loss: 0.0045 | Val Loss: 0.0043 | Val MAE: 0.0634\n",
            "Epoch 023 | Train Loss: 0.0045 | Val Loss: 0.0042 | Val MAE: 0.0633\n",
            "Epoch 024 | Train Loss: 0.0045 | Val Loss: 0.0044 | Val MAE: 0.0644\n",
            "Epoch 025 | Train Loss: 0.0045 | Val Loss: 0.0042 | Val MAE: 0.0632\n",
            "Epoch 026 | Train Loss: 0.0045 | Val Loss: 0.0043 | Val MAE: 0.0644\n",
            "Epoch 027 | Train Loss: 0.0045 | Val Loss: 0.0044 | Val MAE: 0.0655\n",
            "Epoch 028 | Train Loss: 0.0045 | Val Loss: 0.0042 | Val MAE: 0.0625\n",
            "Epoch 029 | Train Loss: 0.0045 | Val Loss: 0.0042 | Val MAE: 0.0630\n",
            "Epoch 030 | Train Loss: 0.0045 | Val Loss: 0.0041 | Val MAE: 0.0605\n",
            "Epoch 031 | Train Loss: 0.0045 | Val Loss: 0.0043 | Val MAE: 0.0634\n",
            "Epoch 032 | Train Loss: 0.0045 | Val Loss: 0.0041 | Val MAE: 0.0624\n",
            "Epoch 033 | Train Loss: 0.0045 | Val Loss: 0.0042 | Val MAE: 0.0626\n",
            "Epoch 034 | Train Loss: 0.0045 | Val Loss: 0.0042 | Val MAE: 0.0630\n",
            "Epoch 035 | Train Loss: 0.0045 | Val Loss: 0.0040 | Val MAE: 0.0609\n",
            "Epoch 036 | Train Loss: 0.0045 | Val Loss: 0.0042 | Val MAE: 0.0638\n",
            "Epoch 037 | Train Loss: 0.0045 | Val Loss: 0.0042 | Val MAE: 0.0648\n",
            "Epoch 038 | Train Loss: 0.0045 | Val Loss: 0.0044 | Val MAE: 0.0655\n",
            "Epoch 039 | Train Loss: 0.0045 | Val Loss: 0.0042 | Val MAE: 0.0622\n",
            "Epoch 040 | Train Loss: 0.0045 | Val Loss: 0.0040 | Val MAE: 0.0602\n",
            "Epoch 041 | Train Loss: 0.0045 | Val Loss: 0.0041 | Val MAE: 0.0607\n",
            "Epoch 042 | Train Loss: 0.0045 | Val Loss: 0.0040 | Val MAE: 0.0600\n",
            "Epoch 043 | Train Loss: 0.0045 | Val Loss: 0.0042 | Val MAE: 0.0613\n",
            "Epoch 044 | Train Loss: 0.0045 | Val Loss: 0.0044 | Val MAE: 0.0638\n",
            "Epoch 045 | Train Loss: 0.0045 | Val Loss: 0.0038 | Val MAE: 0.0579\n",
            "Epoch 046 | Train Loss: 0.0045 | Val Loss: 0.0040 | Val MAE: 0.0618\n",
            "Epoch 047 | Train Loss: 0.0045 | Val Loss: 0.0041 | Val MAE: 0.0635\n",
            "Epoch 048 | Train Loss: 0.0045 | Val Loss: 0.0043 | Val MAE: 0.0625\n",
            "Epoch 049 | Train Loss: 0.0045 | Val Loss: 0.0042 | Val MAE: 0.0613\n",
            "Epoch 050 | Train Loss: 0.0045 | Val Loss: 0.0042 | Val MAE: 0.0618\n",
            "Epoch 051 | Train Loss: 0.0045 | Val Loss: 0.0039 | Val MAE: 0.0593\n",
            "Epoch 052 | Train Loss: 0.0045 | Val Loss: 0.0040 | Val MAE: 0.0600\n",
            "Epoch 053 | Train Loss: 0.0045 | Val Loss: 0.0040 | Val MAE: 0.0610\n",
            "Epoch 054 | Train Loss: 0.0045 | Val Loss: 0.0042 | Val MAE: 0.0633\n",
            "Epoch 055 | Train Loss: 0.0045 | Val Loss: 0.0042 | Val MAE: 0.0638\n",
            "Epoch 056 | Train Loss: 0.0045 | Val Loss: 0.0042 | Val MAE: 0.0631\n",
            "Epoch 057 | Train Loss: 0.0045 | Val Loss: 0.0044 | Val MAE: 0.0664\n",
            "Epoch 058 | Train Loss: 0.0045 | Val Loss: 0.0044 | Val MAE: 0.0655\n",
            "Epoch 059 | Train Loss: 0.0045 | Val Loss: 0.0046 | Val MAE: 0.0671\n",
            "Epoch 060 | Train Loss: 0.0045 | Val Loss: 0.0041 | Val MAE: 0.0623\n",
            "Epoch 061 | Train Loss: 0.0045 | Val Loss: 0.0044 | Val MAE: 0.0657\n",
            "Epoch 062 | Train Loss: 0.0045 | Val Loss: 0.0041 | Val MAE: 0.0613\n",
            "Epoch 063 | Train Loss: 0.0045 | Val Loss: 0.0046 | Val MAE: 0.0653\n",
            "Epoch 064 | Train Loss: 0.0045 | Val Loss: 0.0043 | Val MAE: 0.0643\n",
            "Epoch 065 | Train Loss: 0.0045 | Val Loss: 0.0048 | Val MAE: 0.0685\n",
            "Epoch 066 | Train Loss: 0.0045 | Val Loss: 0.0040 | Val MAE: 0.0615\n",
            "Epoch 067 | Train Loss: 0.0045 | Val Loss: 0.0043 | Val MAE: 0.0641\n",
            "Epoch 068 | Train Loss: 0.0045 | Val Loss: 0.0042 | Val MAE: 0.0631\n",
            "Epoch 069 | Train Loss: 0.0045 | Val Loss: 0.0043 | Val MAE: 0.0634\n",
            "Epoch 070 | Train Loss: 0.0045 | Val Loss: 0.0042 | Val MAE: 0.0643\n",
            "Epoch 071 | Train Loss: 0.0045 | Val Loss: 0.0044 | Val MAE: 0.0666\n",
            "Epoch 072 | Train Loss: 0.0045 | Val Loss: 0.0044 | Val MAE: 0.0647\n",
            "Epoch 073 | Train Loss: 0.0045 | Val Loss: 0.0044 | Val MAE: 0.0651\n",
            "Epoch 074 | Train Loss: 0.0045 | Val Loss: 0.0041 | Val MAE: 0.0615\n",
            "Epoch 075 | Train Loss: 0.0045 | Val Loss: 0.0044 | Val MAE: 0.0653\n",
            "Epoch 076 | Train Loss: 0.0045 | Val Loss: 0.0046 | Val MAE: 0.0671\n",
            "Epoch 077 | Train Loss: 0.0045 | Val Loss: 0.0046 | Val MAE: 0.0671\n",
            "Epoch 078 | Train Loss: 0.0045 | Val Loss: 0.0044 | Val MAE: 0.0633\n",
            "Epoch 079 | Train Loss: 0.0045 | Val Loss: 0.0045 | Val MAE: 0.0652\n",
            "Epoch 080 | Train Loss: 0.0045 | Val Loss: 0.0044 | Val MAE: 0.0656\n",
            "Epoch 081 | Train Loss: 0.0045 | Val Loss: 0.0044 | Val MAE: 0.0633\n",
            "Epoch 082 | Train Loss: 0.0045 | Val Loss: 0.0040 | Val MAE: 0.0598\n",
            "Epoch 083 | Train Loss: 0.0045 | Val Loss: 0.0043 | Val MAE: 0.0639\n",
            "Epoch 084 | Train Loss: 0.0045 | Val Loss: 0.0038 | Val MAE: 0.0592\n",
            "Epoch 085 | Train Loss: 0.0045 | Val Loss: 0.0041 | Val MAE: 0.0622\n",
            "Epoch 086 | Train Loss: 0.0045 | Val Loss: 0.0042 | Val MAE: 0.0628\n",
            "Epoch 087 | Train Loss: 0.0045 | Val Loss: 0.0042 | Val MAE: 0.0644\n",
            "Epoch 088 | Train Loss: 0.0045 | Val Loss: 0.0046 | Val MAE: 0.0670\n",
            "Epoch 089 | Train Loss: 0.0045 | Val Loss: 0.0044 | Val MAE: 0.0648\n",
            "Epoch 090 | Train Loss: 0.0045 | Val Loss: 0.0043 | Val MAE: 0.0621\n",
            "Epoch 091 | Train Loss: 0.0045 | Val Loss: 0.0042 | Val MAE: 0.0626\n",
            "Epoch 092 | Train Loss: 0.0045 | Val Loss: 0.0040 | Val MAE: 0.0599\n",
            "Epoch 093 | Train Loss: 0.0045 | Val Loss: 0.0041 | Val MAE: 0.0603\n",
            "Epoch 094 | Train Loss: 0.0045 | Val Loss: 0.0042 | Val MAE: 0.0622\n",
            "Epoch 095 | Train Loss: 0.0045 | Val Loss: 0.0047 | Val MAE: 0.0665\n",
            "Epoch 096 | Train Loss: 0.0045 | Val Loss: 0.0045 | Val MAE: 0.0646\n",
            "Epoch 097 | Train Loss: 0.0045 | Val Loss: 0.0045 | Val MAE: 0.0658\n",
            "Epoch 098 | Train Loss: 0.0045 | Val Loss: 0.0045 | Val MAE: 0.0666\n",
            "Epoch 099 | Train Loss: 0.0045 | Val Loss: 0.0041 | Val MAE: 0.0618\n",
            "Epoch 100 | Train Loss: 0.0045 | Val Loss: 0.0042 | Val MAE: 0.0629\n",
            "Epoch 101 | Train Loss: 0.0045 | Val Loss: 0.0040 | Val MAE: 0.0607\n",
            "Epoch 102 | Train Loss: 0.0045 | Val Loss: 0.0041 | Val MAE: 0.0619\n",
            "Epoch 103 | Train Loss: 0.0045 | Val Loss: 0.0039 | Val MAE: 0.0593\n",
            "Epoch 104 | Train Loss: 0.0045 | Val Loss: 0.0043 | Val MAE: 0.0623\n",
            "Epoch 105 | Train Loss: 0.0045 | Val Loss: 0.0041 | Val MAE: 0.0613\n",
            "Epoch 106 | Train Loss: 0.0045 | Val Loss: 0.0041 | Val MAE: 0.0620\n",
            "Epoch 107 | Train Loss: 0.0045 | Val Loss: 0.0048 | Val MAE: 0.0688\n",
            "Epoch 108 | Train Loss: 0.0045 | Val Loss: 0.0046 | Val MAE: 0.0687\n",
            "Epoch 109 | Train Loss: 0.0045 | Val Loss: 0.0044 | Val MAE: 0.0641\n",
            "Epoch 110 | Train Loss: 0.0045 | Val Loss: 0.0042 | Val MAE: 0.0631\n",
            "Epoch 111 | Train Loss: 0.0045 | Val Loss: 0.0040 | Val MAE: 0.0617\n",
            "Epoch 112 | Train Loss: 0.0045 | Val Loss: 0.0043 | Val MAE: 0.0643\n",
            "Epoch 113 | Train Loss: 0.0045 | Val Loss: 0.0042 | Val MAE: 0.0619\n",
            "Epoch 114 | Train Loss: 0.0045 | Val Loss: 0.0044 | Val MAE: 0.0633\n",
            "Epoch 115 | Train Loss: 0.0045 | Val Loss: 0.0039 | Val MAE: 0.0588\n",
            "Epoch 116 | Train Loss: 0.0045 | Val Loss: 0.0040 | Val MAE: 0.0604\n",
            "Epoch 117 | Train Loss: 0.0045 | Val Loss: 0.0043 | Val MAE: 0.0643\n",
            "Epoch 118 | Train Loss: 0.0045 | Val Loss: 0.0045 | Val MAE: 0.0676\n",
            "Epoch 119 | Train Loss: 0.0045 | Val Loss: 0.0046 | Val MAE: 0.0666\n",
            "Epoch 120 | Train Loss: 0.0045 | Val Loss: 0.0043 | Val MAE: 0.0634\n",
            "Epoch 121 | Train Loss: 0.0045 | Val Loss: 0.0042 | Val MAE: 0.0619\n",
            "Epoch 122 | Train Loss: 0.0045 | Val Loss: 0.0045 | Val MAE: 0.0655\n",
            "Epoch 123 | Train Loss: 0.0045 | Val Loss: 0.0045 | Val MAE: 0.0649\n",
            "Epoch 124 | Train Loss: 0.0045 | Val Loss: 0.0045 | Val MAE: 0.0664\n",
            "Epoch 125 | Train Loss: 0.0045 | Val Loss: 0.0044 | Val MAE: 0.0651\n",
            "Epoch 126 | Train Loss: 0.0045 | Val Loss: 0.0047 | Val MAE: 0.0669\n",
            "Epoch 127 | Train Loss: 0.0045 | Val Loss: 0.0047 | Val MAE: 0.0666\n",
            "Epoch 128 | Train Loss: 0.0045 | Val Loss: 0.0046 | Val MAE: 0.0667\n",
            "Epoch 129 | Train Loss: 0.0045 | Val Loss: 0.0044 | Val MAE: 0.0654\n",
            "Epoch 130 | Train Loss: 0.0045 | Val Loss: 0.0041 | Val MAE: 0.0608\n",
            "Epoch 131 | Train Loss: 0.0045 | Val Loss: 0.0044 | Val MAE: 0.0660\n",
            "Epoch 132 | Train Loss: 0.0045 | Val Loss: 0.0044 | Val MAE: 0.0632\n",
            "Epoch 133 | Train Loss: 0.0045 | Val Loss: 0.0042 | Val MAE: 0.0602\n",
            "Epoch 134 | Train Loss: 0.0045 | Val Loss: 0.0042 | Val MAE: 0.0616\n",
            "Epoch 135 | Train Loss: 0.0045 | Val Loss: 0.0042 | Val MAE: 0.0618\n",
            "Epoch 136 | Train Loss: 0.0045 | Val Loss: 0.0042 | Val MAE: 0.0625\n",
            "Epoch 137 | Train Loss: 0.0045 | Val Loss: 0.0042 | Val MAE: 0.0620\n",
            "Epoch 138 | Train Loss: 0.0045 | Val Loss: 0.0044 | Val MAE: 0.0628\n",
            "Epoch 139 | Train Loss: 0.0045 | Val Loss: 0.0040 | Val MAE: 0.0614\n",
            "Epoch 140 | Train Loss: 0.0045 | Val Loss: 0.0043 | Val MAE: 0.0640\n",
            "Epoch 141 | Train Loss: 0.0045 | Val Loss: 0.0043 | Val MAE: 0.0637\n",
            "Epoch 142 | Train Loss: 0.0045 | Val Loss: 0.0045 | Val MAE: 0.0659\n",
            "Epoch 143 | Train Loss: 0.0045 | Val Loss: 0.0043 | Val MAE: 0.0649\n",
            "Epoch 144 | Train Loss: 0.0045 | Val Loss: 0.0041 | Val MAE: 0.0618\n",
            "Epoch 145 | Train Loss: 0.0045 | Val Loss: 0.0042 | Val MAE: 0.0607\n",
            "Epoch 146 | Train Loss: 0.0045 | Val Loss: 0.0042 | Val MAE: 0.0630\n",
            "Epoch 147 | Train Loss: 0.0045 | Val Loss: 0.0038 | Val MAE: 0.0579\n",
            "Epoch 148 | Train Loss: 0.0045 | Val Loss: 0.0040 | Val MAE: 0.0613\n",
            "Epoch 149 | Train Loss: 0.0045 | Val Loss: 0.0045 | Val MAE: 0.0659\n",
            "Epoch 150 | Train Loss: 0.0045 | Val Loss: 0.0041 | Val MAE: 0.0618\n",
            "Epoch 151 | Train Loss: 0.0045 | Val Loss: 0.0047 | Val MAE: 0.0664\n",
            "Epoch 152 | Train Loss: 0.0045 | Val Loss: 0.0050 | Val MAE: 0.0691\n",
            "Epoch 153 | Train Loss: 0.0045 | Val Loss: 0.0041 | Val MAE: 0.0613\n",
            "Epoch 154 | Train Loss: 0.0045 | Val Loss: 0.0042 | Val MAE: 0.0631\n",
            "Epoch 155 | Train Loss: 0.0045 | Val Loss: 0.0044 | Val MAE: 0.0657\n",
            "Epoch 156 | Train Loss: 0.0045 | Val Loss: 0.0042 | Val MAE: 0.0627\n",
            "Epoch 157 | Train Loss: 0.0045 | Val Loss: 0.0041 | Val MAE: 0.0633\n",
            "Epoch 158 | Train Loss: 0.0045 | Val Loss: 0.0045 | Val MAE: 0.0665\n",
            "Epoch 159 | Train Loss: 0.0045 | Val Loss: 0.0039 | Val MAE: 0.0601\n",
            "Epoch 160 | Train Loss: 0.0045 | Val Loss: 0.0043 | Val MAE: 0.0632\n",
            "Epoch 161 | Train Loss: 0.0045 | Val Loss: 0.0045 | Val MAE: 0.0658\n",
            "Epoch 162 | Train Loss: 0.0045 | Val Loss: 0.0043 | Val MAE: 0.0641\n",
            "Epoch 163 | Train Loss: 0.0045 | Val Loss: 0.0048 | Val MAE: 0.0702\n",
            "Epoch 164 | Train Loss: 0.0045 | Val Loss: 0.0047 | Val MAE: 0.0694\n",
            "Epoch 165 | Train Loss: 0.0045 | Val Loss: 0.0045 | Val MAE: 0.0656\n",
            "Epoch 166 | Train Loss: 0.0045 | Val Loss: 0.0044 | Val MAE: 0.0632\n",
            "Epoch 167 | Train Loss: 0.0045 | Val Loss: 0.0041 | Val MAE: 0.0614\n",
            "Epoch 168 | Train Loss: 0.0045 | Val Loss: 0.0041 | Val MAE: 0.0611\n",
            "Epoch 169 | Train Loss: 0.0045 | Val Loss: 0.0042 | Val MAE: 0.0637\n",
            "Epoch 170 | Train Loss: 0.0045 | Val Loss: 0.0045 | Val MAE: 0.0662\n",
            "Epoch 171 | Train Loss: 0.0045 | Val Loss: 0.0049 | Val MAE: 0.0709\n",
            "Epoch 172 | Train Loss: 0.0045 | Val Loss: 0.0044 | Val MAE: 0.0660\n",
            "Epoch 173 | Train Loss: 0.0045 | Val Loss: 0.0042 | Val MAE: 0.0621\n",
            "Epoch 174 | Train Loss: 0.0045 | Val Loss: 0.0041 | Val MAE: 0.0605\n",
            "Epoch 175 | Train Loss: 0.0045 | Val Loss: 0.0040 | Val MAE: 0.0601\n",
            "Epoch 176 | Train Loss: 0.0045 | Val Loss: 0.0040 | Val MAE: 0.0592\n",
            "Epoch 177 | Train Loss: 0.0045 | Val Loss: 0.0044 | Val MAE: 0.0647\n",
            "Epoch 178 | Train Loss: 0.0045 | Val Loss: 0.0040 | Val MAE: 0.0602\n",
            "Epoch 179 | Train Loss: 0.0045 | Val Loss: 0.0041 | Val MAE: 0.0616\n",
            "Epoch 180 | Train Loss: 0.0045 | Val Loss: 0.0045 | Val MAE: 0.0680\n",
            "Epoch 181 | Train Loss: 0.0045 | Val Loss: 0.0042 | Val MAE: 0.0614\n",
            "Epoch 182 | Train Loss: 0.0045 | Val Loss: 0.0039 | Val MAE: 0.0593\n",
            "Epoch 183 | Train Loss: 0.0045 | Val Loss: 0.0045 | Val MAE: 0.0650\n",
            "Epoch 184 | Train Loss: 0.0045 | Val Loss: 0.0042 | Val MAE: 0.0642\n",
            "Epoch 185 | Train Loss: 0.0045 | Val Loss: 0.0041 | Val MAE: 0.0616\n",
            "Epoch 186 | Train Loss: 0.0045 | Val Loss: 0.0041 | Val MAE: 0.0628\n",
            "Epoch 187 | Train Loss: 0.0045 | Val Loss: 0.0042 | Val MAE: 0.0617\n",
            "Epoch 188 | Train Loss: 0.0045 | Val Loss: 0.0043 | Val MAE: 0.0630\n",
            "Epoch 189 | Train Loss: 0.0045 | Val Loss: 0.0044 | Val MAE: 0.0642\n",
            "Epoch 190 | Train Loss: 0.0045 | Val Loss: 0.0048 | Val MAE: 0.0682\n",
            "Epoch 191 | Train Loss: 0.0045 | Val Loss: 0.0048 | Val MAE: 0.0688\n",
            "Epoch 192 | Train Loss: 0.0045 | Val Loss: 0.0046 | Val MAE: 0.0653\n",
            "Epoch 193 | Train Loss: 0.0045 | Val Loss: 0.0044 | Val MAE: 0.0644\n",
            "Epoch 194 | Train Loss: 0.0045 | Val Loss: 0.0041 | Val MAE: 0.0614\n",
            "Epoch 195 | Train Loss: 0.0045 | Val Loss: 0.0042 | Val MAE: 0.0635\n",
            "Epoch 196 | Train Loss: 0.0045 | Val Loss: 0.0045 | Val MAE: 0.0658\n",
            "Epoch 197 | Train Loss: 0.0045 | Val Loss: 0.0043 | Val MAE: 0.0639\n",
            "Epoch 198 | Train Loss: 0.0045 | Val Loss: 0.0043 | Val MAE: 0.0634\n",
            "Epoch 199 | Train Loss: 0.0045 | Val Loss: 0.0038 | Val MAE: 0.0578\n",
            "Epoch 200 | Train Loss: 0.0045 | Val Loss: 0.0047 | Val MAE: 0.0671\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, 'mdl.pth')"
      ],
      "metadata": {
        "id": "-rq12uyKEPhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(model, preprocessor, new_data):\n",
        "    new_df = pd.DataFrame(new_data)\n",
        "    new_df.fillna(0, inplace=True)\n",
        "\n",
        "    for col in CAT_COLS:\n",
        "        new_df[col] = new_df[col].astype(str).replace('0', 'MISSING')\n",
        "        le = preprocessor.label_encoders[col]\n",
        "        new_df[col] = new_df[col].apply(lambda x: x if x in le.classes_ else 'MISSING')\n",
        "        new_df[col] = le.transform(new_df[col])\n",
        "\n",
        "    num_cols = new_df.columns[~new_df.columns.isin(CAT_COLS)]\n",
        "    if len(num_cols) > 0:\n",
        "        new_df[num_cols] = preprocessor.scaler.transform(new_df[num_cols])\n",
        "\n",
        "    X_num = torch.tensor(new_df.drop(CAT_COLS, axis=1).values.astype(np.float32))\n",
        "    X_cat = torch.tensor(new_df[CAT_COLS].values.astype(np.int64))\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        pred = model(X_num, X_cat)\n",
        "    return preprocessor.y_scaler.inverse_transform(pred.numpy().reshape(-1, 1)).flatten()\n",
        "\n",
        "new_data = { # Данные формата (колонны)\n",
        "            # blend_id\tcomp1_smiles\tcomp1_type\tcomp1_mass\tcomp1_LogP\tcomp1_TPSA\tcomp1_MolWt\tcomp1_Van_Der_Waals_volume\tcomp1_Fraction_non_rotatable_bonds\tcomp1_num_atoms\t... comp20_LogP\tcomp20_TPSA\tcomp20_MolWt\tcomp20_Van_Der_Waals_volume\tcomp20_Fraction_non_rotatable_bonds\tcomp20_num_atoms\tcomp20_Degree_of_branching\tcomp20_Labute_asa\tcomp20_Mol_mr\toil_property_value\n",
        "            }\n",
        "\n",
        "\n",
        "model.load_state_dict(torch.load('/content/mdl.pth'))\n",
        "model.eval()\n",
        "predictions = inference(model, preprocessor, new_data)\n",
        "print(f\"Predictions: {predictions}\")"
      ],
      "metadata": {
        "id": "FQ0PjvxyGp_J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}