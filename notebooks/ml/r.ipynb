{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bb2b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5287e477",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_and_validate_data(path):\n",
    "    df = pd.read_csv(path)\n",
    "    \n",
    "    assert df['oil_property_value'].isna().sum() == 0, \"NaN в целевом признаке\"\n",
    "    \n",
    "    numeric_cols = ['mass_fraction', 'log_transformed', 'LogP', 'TPSA', \n",
    "                   'MolWt', 'Van_Der_Waals volumeFraction_non_rotatable_bounds',\n",
    "                   'num_atoms', 'Degree_of_branching']\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if df[col].isna().sum() > 0:\n",
    "            median_val = df[col].median()\n",
    "            print(f\"Заполнение {col} медианой: {median_val:.2f}\")\n",
    "            df[col] = df[col].fillna(median_val)\n",
    "    \n",
    "    assert not np.isinf(df[numeric_cols]).any().any(), \"Обнаружены бесконечные значения\"\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95af51e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_encoders(df):\n",
    "    encoders = {\n",
    "        'component': LabelEncoder().fit(df['component_name']),\n",
    "        'type': LabelEncoder().fit(df['component_type_title']),\n",
    "        'smiles': LabelEncoder().fit(df['smiles'])\n",
    "    }\n",
    "    \n",
    "    scaler = StandardScaler().fit(df[numeric_cols])\n",
    "    \n",
    "    return encoders, scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c072a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SafeBlendDataset(Dataset):\n",
    "    def __init__(self, df, encoders, scaler):\n",
    "        self.groups = df.groupby('blend_id')\n",
    "        self.encoders = encoders\n",
    "        self.scaler = scaler\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            group = self.groups.get_group(self.groups.groups.keys()[idx])\n",
    "            features = []\n",
    "            \n",
    "            for _, row in group.iterrows():\n",
    "                component_idx = self.encoders['component'].transform([row['component_name']])[0]\n",
    "                type_idx = self.encoders['type'].transform([row['component_type_title']])[0]\n",
    "                smiles_idx = self.encoders['smiles'].transform([row['smiles']])[0]\n",
    "                \n",
    "                numerical = self.scaler.transform(row[numeric_cols].values.reshape(1, -1))[0]\n",
    "                \n",
    "                features.append(torch.cat([\n",
    "                    torch.tensor([component_idx, type_idx, smiles_idx], dtype=torch.long),\n",
    "                    torch.tensor(numerical, dtype=torch.float32)\n",
    "                ]))\n",
    "            \n",
    "            return torch.stack(features), torch.tensor(group['oil_property_value'].iloc[0], dtype=torch.float32)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка в данных blend_id {group.name}: {str(e)}\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812fd93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SafeTransformer(nn.Module):\n",
    "    def __init__(self, encoders, numeric_dim):\n",
    "        super().__init__()\n",
    "        self.init = lambda m: (nn.init.xavier_normal_(m.weight) if hasattr(m, 'weight') else None\n",
    "        \n",
    "        self.component_embed = nn.Embedding(len(encoders['component'].classes_), 16)\n",
    "        self.type_embed = nn.Embedding(len(encoders['type'].classes_), 8)\n",
    "        self.smiles_embed = nn.Embedding(len(encoders['smiles'].classes_), 32)\n",
    "        self.apply(self.init)\n",
    "        \n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=16+8+32+numeric_dim,\n",
    "                nhead=8,\n",
    "                dim_feedforward=256,\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=4\n",
    "        )\n",
    "        \n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(16+8+32+numeric_dim, 64),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.LayerNorm(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if torch.isnan(x).any():\n",
    "            raise ValueError(\"Обнаружен NaN во входных данных\")\n",
    "            \n",
    "        categorical = x[:, :, :3].long()\n",
    "        numerical = x[:, :, 3:]\n",
    "        \n",
    "        component_emb = self.component_embed(torch.clamp(categorical[:,:,0], 0, self.component_embed.num_embeddings-1))\n",
    "        type_emb = self.type_embed(torch.clamp(categorical[:,:,1], 0, self.type_embed.num_embeddings-1))\n",
    "        smiles_emb = self.smiles_embed(torch.clamp(categorical[:,:,2], 0, self.smiles_embed.num_embeddings-1))\n",
    "        \n",
    "        x = torch.cat([component_emb, type_emb, smiles_emb, numerical], dim=-1)\n",
    "        \n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        return self.regressor(x.mean(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabaf04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_safe(model, dataloader, device, max_grad_norm=1.0):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5)\n",
    "    criterion = nn.HuberLoss()\n",
    "\n",
    "    for epoch in range(100):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            if batch is None: continue  # Пропуск битых данных\n",
    "            \n",
    "            inputs, targets = map(lambda x: x.to(device), batch)\n",
    "            \n",
    "            try:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs.squeeze(), targets)\n",
    "                \n",
    "                if torch.isnan(loss):\n",
    "                    print(f\"Обнаружен NaN loss на эпохе {epoch}\")\n",
    "                    continue\n",
    "                \n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Ошибка обучения: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        scheduler.step(avg_loss)\n",
    "        \n",
    "        if not torch.isnan(torch.tensor(avg_loss)):\n",
    "            print(f\"Epoch {epoch+1}: Loss={avg_loss:.4f}, LR={optimizer.param_groups[0]['lr']:.2e}\")\n",
    "            save_checkpoint(model, epoch, avg_loss)\n",
    "            \n",
    "        else:\n",
    "            print(\"Обучение прервано из-за NaN\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3564116b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_safe(model, sample, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            inputs = sample[0].unsqueeze(0).to(device)\n",
    "            if torch.isnan(inputs).any():\n",
    "                print(\"Обнаружен NaN во входных данных\")\n",
    "                return None\n",
    "            return model(inputs).item()\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка предсказания: {str(e)}\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afbae50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, epoch, loss, save_dir=\"saves\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    checkpoint_path = os.path.join(\n",
    "        save_dir,\n",
    "        f\"checkpoint_epoch_{epoch+1}_loss_{loss:.4f}_{timestamp}.pth\"\n",
    "    )\n",
    "    \n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'loss': loss,\n",
    "        'timestamp': timestamp\n",
    "    }, checkpoint_path)\n",
    "    \n",
    "    print(f\"Checkpoint saved: {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4b59d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    df = load_and_validate_data(\"/content/my_data.csv\")\n",
    "    encoders, scaler = prepare_encoders(df)\n",
    "    \n",
    "    dataset = SafeBlendDataset(df, encoders, scaler)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=16,\n",
    "        collate_fn=lambda x: tuple(filter(None, x))  # Фильтрация битых данных\n",
    "    )\n",
    "    \n",
    "    model = SafeTransformer(encoders, len(numeric_cols)).to(device)\n",
    "    \n",
    "    train_safe(model, dataloader, device)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
